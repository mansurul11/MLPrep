Logistic Regression:
1. linear model (output is the probability of an instance being in a class)
2. sigmoid(wTx+b)=y
3. log loss: sum over all training points -y*log(y) - (1-y)*log(1-y) [cross entropy as well]
4. regularization through: 
  - l2 (ridge) square sum over W's 
    - has derivative and convex
    - set W's to small values
  - l1 (lasso) sum over W's
    - no derivative, concave
    - set W's to zero indicates selection of feature
5. Advantages:
    - computationally faster
    - It makes no assumptions about distributions of classes in feature space.
    - easy to extend for multi class setting
    - interpretable
6. Disadvantages:
    - if not enough data but large number feature cause overfitting
    - assumption of linearity between features and label is not always true
    - always generate linear boundery
    - Cannot handle multi-colinearity
SVM:
1. Support vector machines that aims to separate classes by maximum width margin.
2. Output +1 and -1 as prediction
3. formulated as an optimization problem to minimze 1/|w|^2 here w is the distance between wTx+b=0 and wTx+b=+1 and wTx+b=-1
4. The data points on the line wTx+b=+1 and wTx+b=-1 are called support vectors
5. To handle nonlinera data, kernel is applicable to the data points to transform the to higher dimension where the data points become linearly separable.
6. polynomial, radial-basis are the kernel and RBF is the most popular.
7. To train, we can use hinge loss (max (0, 1-y*y^) y^ = wTx+b) - loss if 0 if y and y^ has the same sign (+1,+1) OR (-1,-1) and 2 is they are opposite)
8. final loss function is : sum over all data points:  max (0, 1-y*y^) + C*epsilon, here epsilon is the noise where prediction is +1 is wTx+b >= 1-epsilon and -1 if
wTx+b =< 1-epsilon, C is regularion param.
9. To use kernel, we can replace x with f(x)  and have wTf(x)+b here f is kernel function.
10. SGD can be using to train the model.
11. To regulate RBF(1/2sigma^2 * (x-x')), we need to tune sigma^2 
12. Another way to formulate is the take a dual of original optimization formulation after using langrange to transform constraint. (dont know much detail)
13. Advantages:
  - Can handle non-linear data through kernel trick
  - handle multi-co-linear data points
14. Disadvantages:
  - computationally expensive for large amount of data
  - can overfit is data is less than features.
  -  prediction is less interpretable as prediction is not interms of probabilty but one can take the score from linear model and convert into probability
Decision Tree:
1. Rule based system where given a feature and its values based on equality/inquality rules the algorithm splits the dataset. one can think is splitting as a creation of
two nodes (childred) in a tree for the parent node representative of the feature in question.
2. The leaf not indicates class or numeric number dependending on classifiction or regression problem.
3. Main challenge is to know at each stage which feature to be used for splitting.
4. Intuition is to find the most discreminative feature as each stage to split the dataset and one way to find that is through information gain measure. Information 
gain is a measure of entropy where higher entropy indicates more ambiguity in the class distribution less discrinative and lower entry low ambiguity higher discrimination
5. Information gain of feature a = entropy(S)-entropy(S|a), we want a with maximum information gain from the current state of the dataset to the splitting version.
7. tree depth is parameter used to regularize the tree
7. Advantages:
  - very interpretable, easy to explain a prediction.
  - easily adapt categorical and numerical feature.
  - low effort on data preprocessing.
8. DisAdvantages:
  - can easily overfit. Backward node prunning may help.
  - changes in training data cause entire tree recreation.
  - 
Random Forest:
1. This is collection of small depth decision trees.
2. Random forest address the overfitting issue of a single deep decision tree.
3. Random forest is an ensemble model trained through bagging.
4. At each step it samples K instances with replacement and build a decision tree.
5. One additional step it does is to also sample a set of features that it uses to build the trees. This help not make the tree correlated.
6. Random forest kind of overfits each tree but different feature set ensures overfitting happend in different direction and aggration of all these tree cancel out each
others overfitting.
7. For classification, it takes majority voting and average for regression.
8. number of estimators/trees and depth of each tree params can be use to regularize.
Advantages:
 - does not overfit
 - interpretable
 - handle missing data
Disadvantages:
  - longer training time
  
Gradient Boosted model:
1. another kind of ensemble model based on boosting.
2. The main idea is that, at every step we want to train a weak learner and in the following step only focus on data points where weak learner is not performing well.
3. Such aggration(addition) of weak learners made up an effective one. f_new = f_old+learning_rate*f_new
4. given y = f(x) is the model, at stage 0 we will train a basic weak learner (always output average of the class label or a very shallow tree) and get y^ (prediction)
now, record error y-y^ and replace target to y-y^ and try to fit another model f on (y-y^), Keep on doing this until there is no change is the error or number of iterations
expires.
5. gradient boosted model can be used for ranking, where given a ranking function f, prediction pairwise prediction y^(i>j) can be derived from a sigmoid 1/1+e(f(xi)-f(xj))
6. we can logloss -ylog(y^)-(1-y)log(1-y^) as cost function. (RankNet)
7. LambdaRank include change (delta) in NDCG when i and j are swapped in the gradient computation of the above log loss cost function.
8.Advantages:
  - Lots of flexibility - can optimize on different loss functions and provides several hyper parameter tuning options that make the function fit very flexible.
  - No data pre-processing required - often works great with categorical and numerical values as is.
  - Handles missing data - imputation not required.
  
9. Disadvantages:
  - Gradient Boosting Models will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting.
  - Computationally expensive - often require many trees (>1000) which can be time and memory exhaustive.
  - The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
  - Less interpretative in nature, although this is easily addressed with various tools.
  
Evaluation:
Classification:
  - Precision: ratio of correct out of all predicted
  - Recall: ratio of correct prediction out of all correct in the set
  - f1: hermonic mean between precision and recall
  - AUC: area under ROC curve measure for different threshold and plotted TPR and FPR.
    - when threshold is zero, both TPR and FPR is zero, indicating bottom left corner
    - when threshold is 1, both TPR and FPR is 1, indicating top right corner
  -PR curve - helps to determine what threshold to use for nominating a prediction as positive.
Regression:
  - R-Square: measure how good the prediction is compared to mean of the target variable.
  - Root mean square error: root of the mean square error (stable than MSE as MSE can explore in case of an outlire.)
Ranking:
  Mean Reciprocal rank: measure what is the rank of first relevant element. measured by mean(1/rank of first relevant element for each data query)
  Precision@K: what fraction of items withing top k is relevant. 
    - does not consider order among the items in the list
  Mean Precision: computed precision @1..K and average.
  Average Mean Precision: Average of mean precision accross queries/users
  CG: cumulative sum of relevance score for each element in the rank list
    - does not penalize of showing not relevant item on top position.
  DCG: 2^(rel_i)/log(i+1) for all ith item in the rank list.
    - hard to interprete and not comparable between users/queries.
  IDCG: DCG for idean rannking
  NDCG: DCG/IDCG: between 0 to 1
    - has problem handling when we dont have complete set of relevant score for the entire ranked list, challeng is how to fill those slots.
    -
Overfitting-underfitting
Regularization
Loss functions
RNN
LSTM GRU
Gradient explosion
Gradient vanishes
Batch normalization
Mini batch gradient descent
Momentum
RMSPorp
Adam
Attention
Activation functions
Loss functions
Two tower
Deep and wide
Pooling (average, min, max)
Alternative Least Square
MF R = UTV
